{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f7JOVheLYY1J"
      },
      "source": [
        "# Word-Based Diacritics Prediction Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "bxM3oLhmA8aG"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import os\n",
        "import pickle\n",
        "import re\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding, LSTM, Dense, Bidirectional\n",
        "from keras.utils import to_categorical"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ZtIHxQnExuu"
      },
      "source": [
        "### Get input data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vyJcHFhHB5Bk",
        "outputId": "0ed06667-596f-4734-d51c-32607267a70c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[['ﺽﺮﻌﻣ', 'ﻥﺁﺮﻘﻠﻟ', 'ﻦﻄﻨﺷﺍﻮﺑ'], ['ﻢﻴﻘﻳ', 'ﻒﺤﺘﻣ', 'ﺚﻴﻤﺳ', 'ﻥﺎﻴﻧﻮﺳ', 'ﻦﻄﻨﺷﺍﻮﺑ', 'ﻥﻭﺎﻌﺘﻟﺎﺑ', 'ﻊﻣ', 'ﻒﺤﺘﻤﻟﺍ', 'ﻲﻛﺮﺘﻟﺍ', 'ﻦﻔﻠﻟ', 'ﻲﻣﻼﺳﻹﺍ', 'ﻲﻓ', 'ﻝﻮﺒﻨﻄﺳﺇ', 'ﺎﺿﺮﻌﻣ', 'ﻥﺁﺮﻘﻠﻟ', 'ﻢﻳﺮﻜﻟﺍ', 'ﻮﻫ', 'ﻝﻭﻷﺍ', 'ﺮﺒﻛﻷﺍﻭ', 'ﻦﻣ', 'ﻪﻋﻮﻧ', 'ﻲﻓ', 'ﺕﺎﻳﻻﻮﻟﺍ', 'ﺓﺪﺤﺘﻤﻟﺍ', 'ﺖﺤﺗ', 'ﻢﺳﺍ', 'ﻦﻓ'], ['ﻥﺁﺮﻘﻟﺍ', 'ﺢﺘﺘﻓﺍﻭ', 'ﺽﺮﻌﻤﻟﺍ', 'ﺮﻬﺸﻟﺍ', 'ﻲﺿﺎﻤﻟﺍ', 'ﻢﻀﻳﻭ', 'ﻒﺣﺎﺼﻣ'], ['ﺓﺭﺩﺎﻧ', 'ﺎﻬﻀﻌﺑ', 'ﺽﺮﻌﻳ', 'ﻝﻭﻷ', 'ﺓﺮﻣ', 'ﺝﺭﺎﺧ'], ['ﺽﺮﻌﻳﻭ', 'ﻲﻓ', 'ﺔﺤﻨﺟﺃ', 'ﺽﺮﻌﻤﻟﺍ', 'ﻱﺬﻟﺍ', 'ﻕﺮﻐﺘﺳﺍ', 'ﺩﺍﺪﻋﻹﺍ', 'ﻪﻤﻴﻈﻨﺘﻟ', 'ﺖﺳ', 'ﺕﺍﻮﻨﺳ', 'ﺮﺜﻛﺃ', 'ﻦﻣ', 'ﻦﻴﺘﺳ', 'ﺎﻔﺤﺼﻣ', 'ﺎﻃﻮﻄﺨﻣ', 'ﺩﻮﻌﺗ', 'ﺎﻬﻟﻮﺻﺃ', 'ﻰﻟﺇ', 'ﺎﻴﻛﺮﺗ', 'ﻥﺎﺘﺴﻧﺎﻐﻓﺃﻭ', 'ﻥﺍﺮﻳﺇﻭ', 'ﻝﻭﺩﻭ'], ['ﺔﻴﺑﺮﻋ', 'ﺖﺒﺘﻛﻭ', 'ﺾﻌﺑ', 'ﻩﺬﻫ', 'ﻒﺣﺎﺼﻤﻟﺍ', 'ﺬﻨﻣ', 'ﺮﺜﻛﺃ', 'ﻦﻣ', 'ﻒﻟﺃ'], ['ﻡﺎﻋ', 'ﺔﻓﺎﺿﺇ', 'ﻰﻟﺇ', 'ﺕﺍﺮﺸﻋ', 'ﺕﺎﻃﻮﻄﺨﻤﻟﺍ', 'ﻯﺮﺧﻷﺍ', 'ﻒﺤﺘﻟﺍﻭ', 'ﻊﻄﻘﻟﺍﻭ', 'ﺔﻴﻨﻔﻟﺍ'], ['ﻦﻜﻤﻳﻭ', 'ﺽﺮﻌﻤﻟﺍ', 'ﺮﺋﺍﺰﻟﺍ', 'ﻦﻣ', 'ﻉﻼﻃﻻﺍ', 'ﻰﻠﻋ', 'ﻥﻮﻨﻓ', 'ﺔﺑﺎﺘﻛ', 'ﻥﺁﺮﻘﻟﺍ', 'ﻢﻳﺮﻜﻟﺍ', 'ﺪﺠﻴﻟ', 'ﻪﻣﺎﻣﺃ', 'ﺎﺼﻧ', 'ﺎﺳﺪﻘﻣ', 'ﺍﺪﺣﺍﻭ', 'ﺐﺘﻛ', 'ﻁﻮﻄﺨﺑ', 'ﺓﺩﺪﻌﺘﻣ', 'ﺔﻠﻴﻤﺟ', 'ﺎﻘﻓﻭ', 'ﺱﺭﺍﺪﻤﻟ', 'ﺔﻴﻨﻓ', 'ﺔﻘﻳﺮﻋ', 'ﻲﻓ', 'ﻂﺨﻟﺍ', 'ﺔﻓﺮﺧﺰﻟﺍﻭ', 'ﻢﺳﺮﻟﺍﻭ'], ['ﻡﺪﻘﻳﻭ', 'ﻥﻮﻤﻈﻨﻤﻟﺍ', 'ﺽﺮﻌﻤﻠﻟ', 'ﺓﺭﻮﺻ', 'ﻭﺪﺒﺗ', 'ﺔﻔﻠﺘﺨﻣ', 'ﻦﻋ', 'ﺓﺭﻮﺼﻟﺍ', 'ﺔﻴﻄﻤﻨﻟﺍ', 'ﺔﻘﺒﺴﻤﻟﺍ', 'ﻯﺪﻟ', 'ﺾﻌﺒﻟﺍ', 'ﻲﻓ', 'ﺎﻜﻳﺮﻣﺃ', 'ﻦﻋ', 'ﻥﺁﺮﻘﻟﺍ', 'ﻡﻼﺳﻹﺍﻭ'], ['ﺎﻣﻮﻤﻋ', 'ﻚﻟﺫﻭ', 'ﻲﻓ', 'ﺔﻟﻭﺎﺤﻣ', 'ﺓﺭﺎﺛﻹ', 'ﺭﺍﻮﺤﻟﺍ', 'ﺵﺎﻘﻨﻟﺍﻭ', 'ﺢﻴﺤﺼﺗﻭ', 'ﺾﻌﺑ'], ['ﻮﻜﺴﻧﻮﻴﻟﺍ', 'ﺔﻐﻠﻟﺍ', 'ﺔﻴﺑﺮﻌﻟﺍ', 'ﺔﻘﻠﺣ', 'ﻞﺻﻭ', 'ﻦﻴﺑ'], ['ﺖﻟﺎﻗ', 'ﺓﺮﻳﺪﻤﻟﺍ', 'ﺔﻣﺎﻌﻟﺍ', 'ﻮﻜﺴﻧﻮﻴﻠﻟ', 'ﺎﻨﻳﺮﻳﺇ', 'ﺎﻓﻮﻛﻮﺑ', 'ﻥﺇ', 'ﺔﻐﻠﻟﺍ', 'ﺔﻴﺑﺮﻌﻟﺍ', 'ﺔﻘﻠﺣ', 'ﻞﺻﻭ', 'ﻦﻴﺑ', 'ﺕﺎﻓﺎﻘﺜﻟﺍ', 'ﺔﻠﻴﺳﻭﻭ', 'ﺔﻴﻘﻴﻘﺣ', 'ﺀﺍﺮﺛﻹ', 'ﻑﺭﺎﻌﻤﻟﺍ', 'ﺭﺎﻜﻓﻷﺍﻭ', 'ﺕﺍﺭﻮﺼﺘﻟﺍﻭ', 'ﺰﻳﺰﻌﺗﻭ', 'ﻢﻫﺎﻔﺘﻟﺍ', 'ﻦﻣ', 'ﻞﺟﺃ', 'ﻝﻼﺣﺇ'], ['ﺕﺮﺒﺘﻋﺍﻭ', 'ﻥﺃ', 'ﻡﻮﻴﻟﺍ', 'ﻲﻤﻟﺎﻌﻟﺍ', 'ﺔﻐﻠﻟ', 'ﺔﻴﺑﺮﻌﻟﺍ', 'ﺔﺒﺳﺎﻨﻣ', 'ﺪﻴﻛﺄﺘﻟ', 'ﺓﺭﻭﺮﺿ', 'ﺰﻳﺰﻌﺗ', 'ﻉﻮﻨﺘﻟﺍ', 'ﻲﻓﺎﻘﺜﻟﺍ', 'ﻱﻮﻐﻠﻟﺍﻭ', 'ﻲﻓ', 'ﻊﻴﻤﺟ', 'ﺀﺎﺟﺭﺃ', 'ﺓﺭﻮﻤﻌﻤﻟﺍ', 'ﺭﺎﺒﺘﻋﺎﺑ', 'ﻪﻧﺃ', 'ﺪﺣﺃ', 'ﺮﺻﺎﻨﻌﻟﺍ', 'ﺔﻴﺴﻴﺋﺮﻟﺍ', 'ﺓﻭﺮﺜﻠﻟ', 'ﺔﻴﻓﺎﻘﺜﻟﺍ', 'ﻲﺘﻟﺍ', 'ﺎﻬﻜﻠﻤﺗ'], ['ﻲﻓﻭ', 'ﻥﺎﻴﺑ', 'ﻪﺗﺭﺪﺻﺃ', 'ﻩﺬﻬﺑ', 'ﺔﺒﺳﺎﻨﻤﻟﺍ', 'ﺖﺤﺿﻭﺃ', 'ﺎﻓﻮﻛﻮﺑ', 'ﻥﺃ', 'ﺀﺎﻔﺘﺣﻻﺍ', 'ﻲﻤﻟﺎﻌﻟﺍ', 'ﺔﻐﻠﻟﺎﺑ', 'ﺔﻴﺑﺮﻌﻟﺍ', 'ﻲﻓ', 'ﺮﺒﻤﺴﻳﺩ', 'ﻥﻮﻧﺎﻛ', 'ﻝﻭﻷﺍ', 'ﻦﻣ', 'ﻞﻛ', 'ﻡﺎﻋ', 'ﻞﺜﻤﻳ', 'ﺍﺭﺍﺮﻗﺇ', 'ﺎﻬﺗﺎﻤﻫﺎﺴﻤﺑ', 'ﺔﻠﻴﻠﺠﻟﺍ', 'ﺔﻠﺋﺎﻬﻟﺍﻭ', 'ﻲﻓ', 'ﺀﺍﺮﺛﺇ', 'ﻡﻮﻠﻌﻟﺍ', 'ﺔﻓﺎﻘﺜﻟﺍﻭ', 'ﺔﻴﻤﻟﺎﻌﻟﺍ', 'ﺎﻬﻨﻣﻭ', 'ﺔﻔﺴﻠﻔﻟﺍ', 'ﺏﺍﺩﻵﺍﻭ'], ['ﺕﺭﺎﺷﺃﻭ', 'ﻰﻟﺇ', 'ﻥﺃ', 'ﺓﺰﺋﺎﺟ', 'ﻮﻜﺴﻧﻮﻴﻟﺍ', 'ﺔﻗﺭﺎﺸﻟﺍ', 'ﺔﻓﺎﻘﺜﻠﻟ', 'ﺔﻴﺑﺮﻌﻟﺍ', 'ﺢﻴﺘﺗ', 'ﻲﻓ', 'ﻞﻛ', 'ﻡﺎﻋ', 'ﺓﺩﺎﺷﻹﺍ', 'ﻝﺎﻤﻋﻷﺎﺑ', 'ﺕﺍﺯﺎﺠﻧﻹﺍﻭ', 'ﺓﺪﻳﺮﻔﻟﺍ', 'ﻲﺘﻟﺍ', 'ﻡﺪﺨﺗ', 'ﺏﺩﻷﺍ', 'ﺔﻓﺎﻘﺜﻟﺍﻭ', 'ﻦﻴﻴﺑﺮﻌﻟﺍ', 'ﻰﻠﻋ', 'ﻕﺎﻄﻧ'], ['ﺕﺩﺪﺷﻭ', 'ﻰﻠﻋ', 'ﻡﺎﻤﺘﻫﺍ', 'ﻮﻜﺴﻧﻮﻴﻟﺍ', 'ﺔﻳﺩﺪﻌﺘﻟﺎﺑ', 'ﺔﻳﻮﻐﻠﻟﺍ', 'ﻥﺎﻘﺗﺇﻭ', 'ﺕﺎﻐﻠﻟﺍ', 'ﻲﻓ', 'ﻞﻇ', 'ﺔﻤﻟﻮﻌﻟﺍ', 'ﺓﺪﻳﺍﺰﺘﻤﻟﺍ', 'ﻲﺘﻟﺍ', 'ﺎﻫﺪﻬﺸﻳ'], ['ﻦﻣﺍﺰﺘﻟﺎﺑﻭ', 'ﻊﻣ', 'ﻡﻮﻴﻟﺍ', 'ﻲﻤﻟﺎﻌﻟﺍ', 'ﺔﻐﻠﻟ', 'ﺔﻴﺑﺮﻌﻟﺍ', 'ﻦﻀﺘﺤﻳ', 'ﺮﻘﻣ', 'ﻮﻜﺴﻧﻮﻴﻟﺍ', 'ﻲﻓ', 'ﺔﻤﺻﺎﻌﻟﺍ', 'ﺔﻴﺴﻧﺮﻔﻟﺍ', 'ﺲﻳﺭﺎﺑ', 'ﺎﺿﺮﻌﻣ', 'ﻦﻋ', 'ﺕﺎﻴﻟﺎﻤﺟ', 'ﻂﺨﻟﺍ'], ['ﺀﻱﺰﺟ', 'ﺪﻳﺰﻳ', 'ﻢﺠﺣ', 'ﺏﻮﺒﺣ', 'ﺢﻤﻘﻟﺍ'], ['ﺭﻮﻃ', 'ﺀﺎﻤﻠﻋ', 'ﺔﻌﻣﺎﺠﺑ', 'ﺩﺭﻮﻔﺴﻛﺃ', 'ﺔﻴﻧﺎﻄﻳﺮﺒﻟﺍ', 'ﺎﺷﺭ', 'ﺎﻴﻋﺍﺭﺯ', 'ﻦﻜﻤﻳ', 'ﻥﺃ', 'ﺪﻳﺰﻳ', 'ﻞﻴﺻﺎﺤﻣ', 'ﺢﻤﻘﻟﺍ', 'ﺭﺍﺪﻘﻤﺑ', 'ﺲﻤﺨﻟﺍ', 'ﻥﻭﺩ', 'ﺔﺟﺎﺣ', 'ﻰﻟﺇ', 'ﺔﻴﻨﻘﺗ', 'ﻞﻳﺪﻌﺘﻟﺍ'], ['ﺪﻘﻓ', 'ﻒﺸﺘﻛﺍ', 'ﻥﻮﺜﺣﺎﺒﻟﺍ', 'ﺎﺌﻳﺰﺟ', 'ﺪﻋﺎﺴﻳ', 'ﺕﺎﺒﻨﻟﺍ', 'ﻲﻓ', 'ﻖﻴﻘﺤﺗ', 'ﻞﻀﻓﺃ', 'ﻡﺍﺪﺨﺘﺳﺍ', 'ﺩﻮﻗﻮﻠﻟ', 'ﻱﺮﻜﺴﻟﺍ', 'ﻱﺬﻟﺍ', 'ﻩﺯﺮﻔﻳ', 'ﻝﻼﺧ', 'ﺔﻴﻠﻤﻋ', 'ﻞﻴﺜﻤﺘﻟﺍ']]\n"
          ]
        }
      ],
      "source": [
        "bare_file = r\"/content/drive/MyDrive/Bin_Data/big-bare-arabic-sentences-list\"\n",
        "with open (bare_file, \"rb\") as f4:\n",
        "  bare_arabic_sentences_list = pickle.load(f4)\n",
        "print (bare_arabic_sentences_list[0:20])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RDpZWSTbErl1"
      },
      "source": [
        "### Get target data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RXxKryt4-eBn",
        "outputId": "9f10bacd-c292-4267-d466-471431068be9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[['وَتَشْتَبِك', 'مَسَارَاتُهُم', 'فِي', 'السَّفَر', 'إِلَى', 'الْعِرَاق', 'خِلَال', 'الرِّحْلَة', 'الْجَمَاعِيَّة', 'الشَّاقَة', 'عَن', 'طَرِيق', 'التَّهْرِيب', 'مِن', 'جَنُوب', 'الْعِرَاق', 'إِلَى', 'الْكُوَيْت', 'الَّتِي', 'كَانَت', 'فِي', 'تِلْك', 'الْفَتْرَة', 'رَمْزًا', 'لِتَحْقِيق'], ['لَكِن', 'رِحْلَة', 'الْأَمَل', 'تَحَوَّلَت', 'إِلَى', 'رِحْلَة', 'مَوْت', 'فِي', 'الصَّحْرَاء', 'قُرْب', 'الْحُدُود', 'الْكُوَيْتِيَّة', 'اخْتِنَاقًا', 'فِي', 'خَزَّان', 'مِيَاه', 'بِشَاحِنَة', 'يَقُودُهَا', 'شَخْص', 'نَمَوْذَج', 'لِلْقِيَادَة'], ['تُرْجِمَت', 'الرِّوَايَة', 'إِلَى', 'لُغَات', 'عَالَمِيَّة', 'وَاكْتَسَبَت', 'شُهْرَتَهَا', 'مِن', 'الْمَكَانَة', 'الْكَبِيرَة', 'الَّتِي', 'كَانَت', 'تَحْظَى', 'بِهَا', 'الْقَضِيَّة', 'الْفِلَسْطِينِيَّة', 'فِي', 'الضَّمِير', 'الْعَالَمِي', 'فِي', 'ذَلِك']]\n"
          ]
        }
      ],
      "source": [
        "in_file= r\"/content/drive/MyDrive/Bin_Data/big-harakat-arabic-sentences-list\"\n",
        "with open (in_file, \"rb\") as f:\n",
        "  arabic_target_sentences_list = pickle.load(f)\n",
        "print (arabic_target_sentences_list[130:133])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JjCbMIe1A5Qh",
        "outputId": "a0c08b84-17f9-41b4-f455-f0e939c222b6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[['1614 1614 1618 1614 1616', '1614 1614 1614 1615 1615', '1616', '1617 1614 1614', '1616 1614', '1618 1616 1614', '1616 1614', '1617 1616 1618 1614', '1618 1614 1614 1616 1617 1614', '1617 1614 1614', '1614', '1614 1616', '1617 1614 1618 1616', '1616', '1614 1615', '1618 1616 1614', '1616 1614', '1618 1615 1614 1618', '1617 1614 1616', '1614 1614', '1616', '1616 1618', '1618 1614 1618 1614', '1614 1618 1611', '1616 1614 1618 1616'], ['1614 1616', '1616 1618 1614', '1618 1614 1614', '1614 1614 1617 1614 1614', '1616 1614', '1616 1618 1614', '1614 1618', '1616', '1617 1614 1618 1614', '1615 1618', '1618 1615 1615', '1618 1615 1614 1618 1616 1617 1614', '1618 1616 1614 1611', '1616', '1614 1617 1614', '1616 1614', '1616 1614 1616 1614', '1614 1615 1615 1614', '1614 1618', '1614 1614 1618 1614', '1616 1618 1616 1614 1614'], ['1615 1618 1616 1614', '1617 1616 1614 1614', '1616 1614', '1615 1614', '1614 1614 1616 1617 1614', '1614 1618 1614 1614 1614', '1615 1618 1614 1614 1614', '1616', '1618 1614 1614 1614', '1618 1614 1616 1614', '1617 1614 1616', '1614 1614', '1614 1618 1614', '1616 1614', '1618 1614 1616 1617 1614', '1618 1616 1614 1618 1616 1616 1617 1614', '1616', '1617 1614 1616', '1618 1614 1614 1616', '1616', '1614 1616']]\n"
          ]
        }
      ],
      "source": [
        "labels_file= r\"/content/drive/MyDrive/Bin_Data/big-harakat-only-unicode-sentences-list\"\n",
        "with open (labels_file, \"rb\") as f3:\n",
        "  unicode_target_sentences_list = pickle.load(f3)\n",
        "print (unicode_target_sentences_list[130:133])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vUyvXNUuZYvo"
      },
      "source": [
        "# Reduce data size to test CPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "RhK22V_yZWeE"
      },
      "outputs": [],
      "source": [
        "small_bare_arabic_sentences_list = bare_arabic_sentences_list[0:20000]\n",
        "small_unicode_target_sentences_list = unicode_target_sentences_list[0:20000]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VuCoXfTcBjLZ"
      },
      "source": [
        "# Tokenization\n",
        "\n",
        "\n",
        "using filtes=None to keep punctuation in the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "2G6JoC2OBhSO"
      },
      "outputs": [],
      "source": [
        "bare_tokenizer = keras.preprocessing.text.Tokenizer(filters=None, oov_token='<OOV>')\n",
        "bare_tokenizer.fit_on_texts(small_bare_arabic_sentences_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "EwuEp-s_JgE3"
      },
      "outputs": [],
      "source": [
        "harakat_tokenizer = keras.preprocessing.text.Tokenizer(filters=None, oov_token='<OOV>')\n",
        "harakat_tokenizer.fit_on_texts(small_unicode_target_sentences_list)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fMGZrBPpE6nB"
      },
      "source": [
        "# Vectorization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "4RDSJyv_VHfm"
      },
      "outputs": [],
      "source": [
        "X_seqs = bare_tokenizer.texts_to_sequences(small_bare_arabic_sentences_list)\n",
        "y_seqs = harakat_tokenizer.texts_to_sequences(small_unicode_target_sentences_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F7gb2acTA38C",
        "outputId": "6891e94d-bcc9-4316-b9c5-ff38b131f034"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[['مَعْرِض', 'للْقُرْآن', 'بِوَاشِنْطُن']]\n",
            "[['1614 1618 1616', '1618 1615 1618', '1616 1614 1616 1618 1615']]\n",
            "[['ﺽﺮﻌﻣ', 'ﻥﺁﺮﻘﻠﻟ', 'ﻦﻄﻨﺷﺍﻮﺑ']]\n"
          ]
        }
      ],
      "source": [
        "# Inspect sequences\n",
        "print (arabic_target_sentences_list[0:1])\n",
        "print (unicode_target_sentences_list[0:1])\n",
        "print (bare_arabic_sentences_list [0:1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zQ2YWrSoRBHu",
        "outputId": "f966a88c-92b6-4ba0-8414-d66c8fa05d86"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Length of longest input sequence: 173\n"
          ]
        }
      ],
      "source": [
        "MAX_LENGTH = len(max(X_seqs, key=len))\n",
        "print(f\"Length of longest input sequence: {MAX_LENGTH}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "SAytd3ZOWd4c"
      },
      "outputs": [],
      "source": [
        "X_padded = keras.preprocessing.sequence.pad_sequences(X_seqs, padding='post',\n",
        "                                                            maxlen=MAX_LENGTH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "XxfHBhhZNFLi"
      },
      "outputs": [],
      "source": [
        "y_padded = keras.preprocessing.sequence.pad_sequences(y_seqs, padding='post',\n",
        "                                                            maxlen=MAX_LENGTH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MQaokFJ5a0vr",
        "outputId": "cca7952e-3eaf-4b5f-ad06-0ef12dda1a39"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " X_padded is a <class 'numpy.ndarray'> of shape (16000, 173)\n",
            " y_padded is a <class 'numpy.ndarray'> of shape (16000, 173)\n"
          ]
        }
      ],
      "source": [
        "print (f\" X_padded is a {type (X_padded)} of shape {np.shape(X_train)}\")\n",
        "print (f\" y_padded is a {type (y_padded)} of shape {np.shape(y_train)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uoBdteS2FtG3"
      },
      "source": [
        "# Splitting To Datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "7wVHLNCvXwK8"
      },
      "outputs": [],
      "source": [
        "X_train, X_temp, y_train, y_temp = train_test_split(X_padded, y_padded, test_size=0.2, random_state=13)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=13)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iR79QEMWaTsx"
      },
      "source": [
        "# Model Configuration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ka4xVLAZAYMl"
      },
      "source": [
        "### Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1aKS-xF6Cl7j",
        "outputId": "147d7ad1-7fb8-44df-f0fd-780f8e76885b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(Number of input sentences:  20000\n",
            "Number of tokens:  38132\n",
            "Number of target sentences:  20000\n",
            "Number of target labels:  2927\n"
          ]
        }
      ],
      "source": [
        "num_of_sentences = len(small_bare_arabic_sentences_list)\n",
        "print (\"(Number of input sentences: \", num_of_sentences)\n",
        "\n",
        "num_of_tokens = len(bare_tokenizer.word_index)+1\n",
        "print (\"Number of tokens: \", num_of_tokens)\n",
        "\n",
        "num_of_target_sentences = len(small_unicode_target_sentences_list)\n",
        "print (\"Number of target sentences: \", num_of_target_sentences )\n",
        "\n",
        "num_of_labels = len(harakat_tokenizer.word_index)+1\n",
        "print (\"Number of target labels: \", num_of_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qXTE8bQLKAbc",
        "outputId": "9f1c5d62-6447-4eca-e1b3-8a93451aa6cd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "X and y should be equal\n",
            "X_train has 16000 sentences, and y_train has 16000\n",
            "X_val has 2000 sentencs, and y_val has 2000\n",
            "X_test has 2000 sentences, and y_test has 2000\n"
          ]
        }
      ],
      "source": [
        "print (\"X and y should be equal\")\n",
        "print(f\"X_train has {len(X_train)} sentences, and y_train has {len(y_train)}\")\n",
        "print(f\"X_val has {len(X_val)} sentencs, and y_val has {len(y_val)}\")\n",
        "print(f\"X_test has {len(X_test)} sentences, and y_test has {len(y_test)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BMsVzm5NAUMJ"
      },
      "source": [
        "### Layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "-fuRzVAJAFkx"
      },
      "outputs": [],
      "source": [
        "batch_size = 40\n",
        "embedding_dim = 128\n",
        "num_of_batches = (num_of_sentences + batch_size) // batch_size\n",
        "num_of_epochs = 18"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oQ4olEUdaTHB"
      },
      "outputs": [],
      "source": [
        "model = keras.models.Sequential()\n",
        "model.add(Embedding(input_dim=num_of_tokens, output_dim=embedding_dim, input_length=MAX_LENGTH))\n",
        "model.add(Bidirectional(LSTM(units=64, return_sequences=True)))\n",
        "model.add(Dense(units=num_of_labels, activation='softmax'))\n",
        "\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Smn4mTj0A3yZ"
      },
      "source": [
        "# Model Training Using CPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "30xnGoLH8C-7",
        "outputId": "a1f2ce34-f686-4a2c-8456-7a42e9982e0f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "63/63 [==============================] - 11s 152ms/step - loss: 0.3094 - accuracy: 0.9442\n",
            "Epoch 1, Training Loss: 0.5999, Validation Loss: 0.3094, Validation Accuracy: 0.9442\n",
            "63/63 [==============================] - 10s 156ms/step - loss: 0.2996 - accuracy: 0.9450\n",
            "Epoch 2, Training Loss: 0.2387, Validation Loss: 0.2996, Validation Accuracy: 0.9450\n",
            "63/63 [==============================] - 22s 342ms/step - loss: 0.2629 - accuracy: 0.9528\n",
            "Epoch 3, Training Loss: 0.2207, Validation Loss: 0.2629, Validation Accuracy: 0.9528\n",
            "63/63 [==============================] - 10s 152ms/step - loss: 0.2206 - accuracy: 0.9610\n",
            "Epoch 4, Training Loss: 0.1897, Validation Loss: 0.2206, Validation Accuracy: 0.9610\n",
            "63/63 [==============================] - 10s 156ms/step - loss: 0.1673 - accuracy: 0.9719\n",
            "Epoch 5, Training Loss: 0.1469, Validation Loss: 0.1673, Validation Accuracy: 0.9719\n",
            "63/63 [==============================] - 10s 155ms/step - loss: 0.1320 - accuracy: 0.9796\n",
            "Epoch 6, Training Loss: 0.1030, Validation Loss: 0.1320, Validation Accuracy: 0.9796\n",
            "63/63 [==============================] - 10s 157ms/step - loss: 0.1130 - accuracy: 0.9828\n",
            "Epoch 7, Training Loss: 0.0729, Validation Loss: 0.1130, Validation Accuracy: 0.9828\n",
            "63/63 [==============================] - 10s 152ms/step - loss: 0.1025 - accuracy: 0.9849\n",
            "Epoch 8, Training Loss: 0.0533, Validation Loss: 0.1025, Validation Accuracy: 0.9849\n",
            "63/63 [==============================] - 10s 151ms/step - loss: 0.0963 - accuracy: 0.9860\n",
            "Epoch 9, Training Loss: 0.0402, Validation Loss: 0.0963, Validation Accuracy: 0.9860\n",
            "63/63 [==============================] - 10s 156ms/step - loss: 0.0940 - accuracy: 0.9862\n",
            "Epoch 10, Training Loss: 0.0312, Validation Loss: 0.0940, Validation Accuracy: 0.9862\n",
            "63/63 [==============================] - 10s 151ms/step - loss: 0.0912 - accuracy: 0.9873\n",
            "Epoch 11, Training Loss: 0.0249, Validation Loss: 0.0912, Validation Accuracy: 0.9873\n",
            "63/63 [==============================] - 10s 152ms/step - loss: 0.0908 - accuracy: 0.9876\n",
            "Epoch 12, Training Loss: 0.0201, Validation Loss: 0.0908, Validation Accuracy: 0.9876\n",
            "63/63 [==============================] - 10s 154ms/step - loss: 0.0911 - accuracy: 0.9878\n",
            "Epoch 13, Training Loss: 0.0164, Validation Loss: 0.0911, Validation Accuracy: 0.9878\n",
            "63/63 [==============================] - 10s 155ms/step - loss: 0.0915 - accuracy: 0.9880\n",
            "Epoch 14, Training Loss: 0.0135, Validation Loss: 0.0915, Validation Accuracy: 0.9880\n",
            "63/63 [==============================] - 10s 157ms/step - loss: 0.0925 - accuracy: 0.9881\n",
            "Epoch 15, Training Loss: 0.0112, Validation Loss: 0.0925, Validation Accuracy: 0.9881\n",
            "63/63 [==============================] - 10s 157ms/step - loss: 0.0941 - accuracy: 0.9882\n",
            "Epoch 16, Training Loss: 0.0093, Validation Loss: 0.0941, Validation Accuracy: 0.9882\n",
            "63/63 [==============================] - 10s 152ms/step - loss: 0.0948 - accuracy: 0.9883\n",
            "Epoch 17, Training Loss: 0.0077, Validation Loss: 0.0948, Validation Accuracy: 0.9883\n",
            "63/63 [==============================] - 10s 157ms/step - loss: 0.0962 - accuracy: 0.9884\n",
            "Epoch 18, Training Loss: 0.0064, Validation Loss: 0.0962, Validation Accuracy: 0.9884\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best Training Loss: 0.0201\n",
            "Best Validation Loss: 0.0908\n"
          ]
        }
      ],
      "source": [
        "# Initializing loss value as infinity so it goes down from there.\n",
        "best_val_loss = float('inf')\n",
        "best_train_loss = float('inf')\n",
        "\n",
        "# Training loop takes time\n",
        "for epoch in range(num_of_epochs):\n",
        "    total_train_loss = 0\n",
        "\n",
        "    for i in range(num_of_batches):\n",
        "        start_idx = i * batch_size\n",
        "        end_idx = (i + 1) * batch_size if (i + 1) * batch_size < num_of_sentences else num_of_sentences\n",
        "\n",
        "        # Extracting a batch of sequences and labels\n",
        "        X_batch = X_train[start_idx:end_idx]\n",
        "        y_batch = y_train[start_idx:end_idx]\n",
        "\n",
        "        # Converting labels to one-hot encodings per batch because that's where the memory errors were raised\n",
        "        y_one_hot_batch = to_categorical(y_batch, num_classes=num_of_labels)\n",
        "\n",
        "        # Usting train_on_batch which returns a list containing [Loss, any-metrics-in-model.complie]\n",
        "        train_metrics = model.train_on_batch(X_batch, y_one_hot_batch)\n",
        "        train_loss = train_metrics[0]\n",
        "        total_train_loss += train_loss\n",
        "\n",
        "    # Calculating average training loss for the epoch\n",
        "    avg_train_loss = total_train_loss / num_of_batches\n",
        "\n",
        "    # Evaluating on the validation set per epoch to compare the scores to the training Loss\n",
        "    val_loss, val_accuracy = model.evaluate(X_val, to_categorical(y_val, num_classes=num_of_labels))\n",
        "    print(f'Epoch {epoch + 1}, Training Loss: {avg_train_loss:.4f}, Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}')\n",
        "\n",
        "# Saving the model\n",
        "model.save(r\"/content/drive/MyDrive/Colab_Notebooks/half_model.h5\")\n",
        "\n",
        "print(f\"Best Training Loss: {best_train_loss:.4f}\")\n",
        "print(f\"Best Validation Loss: {best_val_loss:.4f}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4-13GIvdLXBg"
      },
      "source": [
        "# Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "1cVLo016-iWA"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "my_model = load_model(r\"/content/drive/MyDrive/Colab_Notebooks/half_model.h5\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Ia6HqXWRo8K",
        "outputId": "d3cff5ab-ac22-4c4a-de43-2c5a5e50469f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 - 1s - loss: 0.0769 - accuracy: 0.9855 - 940ms/epoch - 940ms/step\n",
            "1/1 - 0s - loss: 0.0943 - accuracy: 0.9896 - 71ms/epoch - 71ms/step\n",
            "1/1 - 0s - loss: 0.0914 - accuracy: 0.9884 - 136ms/epoch - 136ms/step\n",
            "1/1 - 0s - loss: 0.0637 - accuracy: 0.9879 - 85ms/epoch - 85ms/step\n",
            "1/1 - 0s - loss: 0.1012 - accuracy: 0.9879 - 86ms/epoch - 86ms/step\n",
            "1/1 - 0s - loss: 0.0674 - accuracy: 0.9896 - 71ms/epoch - 71ms/step\n",
            "1/1 - 0s - loss: 0.0682 - accuracy: 0.9890 - 140ms/epoch - 140ms/step\n",
            "1/1 - 0s - loss: 0.0969 - accuracy: 0.9902 - 69ms/epoch - 69ms/step\n",
            "1/1 - 0s - loss: 0.0663 - accuracy: 0.9936 - 78ms/epoch - 78ms/step\n",
            "1/1 - 0s - loss: 0.1196 - accuracy: 0.9855 - 75ms/epoch - 75ms/step\n",
            "1/1 - 0s - loss: 0.0961 - accuracy: 0.9884 - 80ms/epoch - 80ms/step\n",
            "1/1 - 0s - loss: 0.1032 - accuracy: 0.9890 - 79ms/epoch - 79ms/step\n",
            "1/1 - 0s - loss: 0.1062 - accuracy: 0.9890 - 78ms/epoch - 78ms/step\n",
            "1/1 - 0s - loss: 0.0378 - accuracy: 0.9936 - 79ms/epoch - 79ms/step\n",
            "1/1 - 0s - loss: 0.0225 - accuracy: 0.9954 - 80ms/epoch - 80ms/step\n",
            "1/1 - 0s - loss: 0.0732 - accuracy: 0.9902 - 85ms/epoch - 85ms/step\n",
            "1/1 - 0s - loss: 0.1181 - accuracy: 0.9867 - 67ms/epoch - 67ms/step\n",
            "1/1 - 0s - loss: 0.1003 - accuracy: 0.9873 - 82ms/epoch - 82ms/step\n",
            "1/1 - 0s - loss: 0.1373 - accuracy: 0.9867 - 79ms/epoch - 79ms/step\n",
            "1/1 - 0s - loss: 0.0779 - accuracy: 0.9902 - 84ms/epoch - 84ms/step\n",
            "1/1 - 0s - loss: 0.1326 - accuracy: 0.9850 - 73ms/epoch - 73ms/step\n",
            "1/1 - 0s - loss: 0.0511 - accuracy: 0.9919 - 76ms/epoch - 76ms/step\n",
            "1/1 - 0s - loss: 0.1159 - accuracy: 0.9838 - 70ms/epoch - 70ms/step\n",
            "1/1 - 0s - loss: 0.0583 - accuracy: 0.9925 - 132ms/epoch - 132ms/step\n",
            "1/1 - 0s - loss: 0.1095 - accuracy: 0.9873 - 79ms/epoch - 79ms/step\n",
            "1/1 - 0s - loss: 0.0990 - accuracy: 0.9884 - 77ms/epoch - 77ms/step\n",
            "1/1 - 0s - loss: 0.0963 - accuracy: 0.9896 - 85ms/epoch - 85ms/step\n",
            "1/1 - 0s - loss: 0.0906 - accuracy: 0.9873 - 106ms/epoch - 106ms/step\n",
            "1/1 - 0s - loss: 0.1317 - accuracy: 0.9809 - 77ms/epoch - 77ms/step\n",
            "1/1 - 0s - loss: 0.0354 - accuracy: 0.9942 - 73ms/epoch - 73ms/step\n",
            "1/1 - 0s - loss: 0.0738 - accuracy: 0.9884 - 76ms/epoch - 76ms/step\n",
            "1/1 - 0s - loss: 0.0911 - accuracy: 0.9884 - 75ms/epoch - 75ms/step\n",
            "1/1 - 0s - loss: 0.1435 - accuracy: 0.9832 - 82ms/epoch - 82ms/step\n",
            "1/1 - 0s - loss: 0.0543 - accuracy: 0.9913 - 76ms/epoch - 76ms/step\n",
            "1/1 - 0s - loss: 0.0814 - accuracy: 0.9896 - 75ms/epoch - 75ms/step\n",
            "1/1 - 0s - loss: 0.0929 - accuracy: 0.9879 - 73ms/epoch - 73ms/step\n",
            "1/1 - 0s - loss: 0.1042 - accuracy: 0.9844 - 114ms/epoch - 114ms/step\n",
            "1/1 - 0s - loss: 0.0881 - accuracy: 0.9913 - 70ms/epoch - 70ms/step\n",
            "1/1 - 0s - loss: 0.1543 - accuracy: 0.9850 - 71ms/epoch - 71ms/step\n",
            "1/1 - 0s - loss: 0.0599 - accuracy: 0.9902 - 79ms/epoch - 79ms/step\n",
            "1/1 - 0s - loss: 0.0694 - accuracy: 0.9908 - 135ms/epoch - 135ms/step\n",
            "1/1 - 0s - loss: 0.1352 - accuracy: 0.9867 - 88ms/epoch - 88ms/step\n",
            "1/1 - 0s - loss: 0.0516 - accuracy: 0.9931 - 93ms/epoch - 93ms/step\n",
            "1/1 - 0s - loss: 0.1481 - accuracy: 0.9809 - 80ms/epoch - 80ms/step\n",
            "1/1 - 0s - loss: 0.0463 - accuracy: 0.9942 - 122ms/epoch - 122ms/step\n",
            "1/1 - 0s - loss: 0.0705 - accuracy: 0.9908 - 74ms/epoch - 74ms/step\n",
            "1/1 - 0s - loss: 0.0972 - accuracy: 0.9890 - 75ms/epoch - 75ms/step\n",
            "1/1 - 0s - loss: 0.1379 - accuracy: 0.9844 - 72ms/epoch - 72ms/step\n",
            "1/1 - 0s - loss: 0.0599 - accuracy: 0.9931 - 133ms/epoch - 133ms/step\n",
            "1/1 - 0s - loss: 0.0513 - accuracy: 0.9919 - 71ms/epoch - 71ms/step\n",
            "1/1 - 0s - loss: 0.0988 - accuracy: 0.9861 - 72ms/epoch - 72ms/step\n",
            "1/1 - 0s - loss: 0.0557 - accuracy: 0.9919 - 64ms/epoch - 64ms/step\n",
            "1/1 - 0s - loss: 0.0952 - accuracy: 0.9908 - 64ms/epoch - 64ms/step\n",
            "1/1 - 0s - loss: 0.0908 - accuracy: 0.9879 - 84ms/epoch - 84ms/step\n",
            "1/1 - 0s - loss: 0.0942 - accuracy: 0.9879 - 68ms/epoch - 68ms/step\n",
            "1/1 - 0s - loss: 0.1328 - accuracy: 0.9855 - 95ms/epoch - 95ms/step\n",
            "1/1 - 0s - loss: 0.0878 - accuracy: 0.9890 - 75ms/epoch - 75ms/step\n",
            "1/1 - 0s - loss: 0.1619 - accuracy: 0.9821 - 108ms/epoch - 108ms/step\n",
            "1/1 - 0s - loss: 0.0925 - accuracy: 0.9861 - 76ms/epoch - 76ms/step\n",
            "1/1 - 0s - loss: 0.1501 - accuracy: 0.9827 - 79ms/epoch - 79ms/step\n",
            "1/1 - 0s - loss: 0.0901 - accuracy: 0.9879 - 79ms/epoch - 79ms/step\n",
            "1/1 - 0s - loss: 0.1274 - accuracy: 0.9855 - 130ms/epoch - 130ms/step\n",
            "1/1 - 0s - loss: 0.0811 - accuracy: 0.9902 - 80ms/epoch - 80ms/step\n",
            "1/1 - 0s - loss: 0.0629 - accuracy: 0.9931 - 88ms/epoch - 88ms/step\n",
            "1/1 - 0s - loss: 0.0868 - accuracy: 0.9896 - 77ms/epoch - 77ms/step\n",
            "1/1 - 0s - loss: 0.0370 - accuracy: 0.9960 - 143ms/epoch - 143ms/step\n",
            "1/1 - 0s - loss: 0.1137 - accuracy: 0.9873 - 76ms/epoch - 76ms/step\n",
            "1/1 - 0s - loss: 0.0497 - accuracy: 0.9919 - 80ms/epoch - 80ms/step\n",
            "1/1 - 0s - loss: 0.0569 - accuracy: 0.9931 - 86ms/epoch - 86ms/step\n",
            "1/1 - 0s - loss: 0.0610 - accuracy: 0.9925 - 115ms/epoch - 115ms/step\n",
            "1/1 - 0s - loss: 0.0975 - accuracy: 0.9884 - 72ms/epoch - 72ms/step\n",
            "1/1 - 0s - loss: 0.1143 - accuracy: 0.9861 - 74ms/epoch - 74ms/step\n",
            "1/1 - 0s - loss: 0.0541 - accuracy: 0.9931 - 74ms/epoch - 74ms/step\n",
            "1/1 - 0s - loss: 0.0643 - accuracy: 0.9925 - 80ms/epoch - 80ms/step\n",
            "1/1 - 0s - loss: 0.0626 - accuracy: 0.9913 - 71ms/epoch - 71ms/step\n",
            "1/1 - 0s - loss: 0.0502 - accuracy: 0.9908 - 82ms/epoch - 82ms/step\n",
            "1/1 - 0s - loss: 0.1348 - accuracy: 0.9827 - 78ms/epoch - 78ms/step\n",
            "1/1 - 0s - loss: 0.1193 - accuracy: 0.9838 - 71ms/epoch - 71ms/step\n",
            "1/1 - 0s - loss: 0.1611 - accuracy: 0.9821 - 72ms/epoch - 72ms/step\n",
            "1/1 - 0s - loss: 0.1637 - accuracy: 0.9803 - 70ms/epoch - 70ms/step\n",
            "1/1 - 0s - loss: 0.0952 - accuracy: 0.9879 - 65ms/epoch - 65ms/step\n",
            "1/1 - 0s - loss: 0.0465 - accuracy: 0.9902 - 79ms/epoch - 79ms/step\n",
            "1/1 - 0s - loss: 0.1450 - accuracy: 0.9803 - 124ms/epoch - 124ms/step\n",
            "1/1 - 0s - loss: 0.0809 - accuracy: 0.9890 - 72ms/epoch - 72ms/step\n",
            "1/1 - 0s - loss: 0.0735 - accuracy: 0.9884 - 80ms/epoch - 80ms/step\n",
            "1/1 - 0s - loss: 0.1238 - accuracy: 0.9838 - 72ms/epoch - 72ms/step\n",
            "1/1 - 0s - loss: 0.0788 - accuracy: 0.9890 - 86ms/epoch - 86ms/step\n",
            "1/1 - 0s - loss: 0.0568 - accuracy: 0.9913 - 72ms/epoch - 72ms/step\n",
            "1/1 - 0s - loss: 0.1047 - accuracy: 0.9896 - 72ms/epoch - 72ms/step\n",
            "1/1 - 0s - loss: 0.1476 - accuracy: 0.9832 - 71ms/epoch - 71ms/step\n",
            "1/1 - 0s - loss: 0.0810 - accuracy: 0.9879 - 80ms/epoch - 80ms/step\n",
            "1/1 - 0s - loss: 0.0873 - accuracy: 0.9879 - 143ms/epoch - 143ms/step\n",
            "1/1 - 0s - loss: 0.1881 - accuracy: 0.9757 - 74ms/epoch - 74ms/step\n",
            "1/1 - 0s - loss: 0.1131 - accuracy: 0.9879 - 72ms/epoch - 72ms/step\n",
            "1/1 - 0s - loss: 0.1502 - accuracy: 0.9827 - 77ms/epoch - 77ms/step\n",
            "1/1 - 0s - loss: 0.0643 - accuracy: 0.9908 - 102ms/epoch - 102ms/step\n",
            "1/1 - 0s - loss: 0.1290 - accuracy: 0.9861 - 83ms/epoch - 83ms/step\n",
            "1/1 - 0s - loss: 0.0858 - accuracy: 0.9890 - 65ms/epoch - 65ms/step\n",
            "1/1 - 0s - loss: 0.1299 - accuracy: 0.9850 - 64ms/epoch - 64ms/step\n",
            "1/1 - 0s - loss: 0.1263 - accuracy: 0.9867 - 129ms/epoch - 129ms/step\n",
            "1/1 - 0s - loss: 0.0373 - accuracy: 0.9948 - 87ms/epoch - 87ms/step\n",
            "1/1 - 0s - loss: 0.0318 - accuracy: 0.9960 - 74ms/epoch - 74ms/step\n",
            "1/1 - 0s - loss: 0.0619 - accuracy: 0.9925 - 69ms/epoch - 69ms/step\n",
            "1/1 - 0s - loss: 0.0611 - accuracy: 0.9925 - 127ms/epoch - 127ms/step\n",
            "1/1 - 0s - loss: 0.0542 - accuracy: 0.9925 - 71ms/epoch - 71ms/step\n",
            "1/1 - 0s - loss: 0.0803 - accuracy: 0.9890 - 69ms/epoch - 69ms/step\n",
            "1/1 - 0s - loss: 0.0800 - accuracy: 0.9913 - 67ms/epoch - 67ms/step\n",
            "1/1 - 0s - loss: 0.0550 - accuracy: 0.9931 - 97ms/epoch - 97ms/step\n",
            "1/1 - 0s - loss: 0.1580 - accuracy: 0.9815 - 73ms/epoch - 73ms/step\n",
            "1/1 - 0s - loss: 0.1632 - accuracy: 0.9821 - 73ms/epoch - 73ms/step\n",
            "1/1 - 0s - loss: 0.1062 - accuracy: 0.9902 - 74ms/epoch - 74ms/step\n",
            "1/1 - 0s - loss: 0.0188 - accuracy: 0.9971 - 90ms/epoch - 90ms/step\n",
            "1/1 - 0s - loss: 0.0878 - accuracy: 0.9890 - 73ms/epoch - 73ms/step\n",
            "1/1 - 0s - loss: 0.1157 - accuracy: 0.9896 - 71ms/epoch - 71ms/step\n",
            "1/1 - 0s - loss: 0.1072 - accuracy: 0.9873 - 76ms/epoch - 76ms/step\n",
            "1/1 - 0s - loss: 0.0834 - accuracy: 0.9913 - 68ms/epoch - 68ms/step\n",
            "1/1 - 0s - loss: 0.0733 - accuracy: 0.9908 - 121ms/epoch - 121ms/step\n",
            "1/1 - 0s - loss: 0.1280 - accuracy: 0.9855 - 65ms/epoch - 65ms/step\n",
            "1/1 - 0s - loss: 0.0767 - accuracy: 0.9913 - 69ms/epoch - 69ms/step\n",
            "1/1 - 0s - loss: 0.1344 - accuracy: 0.9809 - 68ms/epoch - 68ms/step\n",
            "1/1 - 0s - loss: 0.0648 - accuracy: 0.9919 - 71ms/epoch - 71ms/step\n",
            "1/1 - 0s - loss: 0.0744 - accuracy: 0.9908 - 85ms/epoch - 85ms/step\n",
            "1/1 - 0s - loss: 0.0680 - accuracy: 0.9919 - 69ms/epoch - 69ms/step\n",
            "1/1 - 0s - loss: 0.0620 - accuracy: 0.9936 - 72ms/epoch - 72ms/step\n",
            "1/1 - 0s - loss: 0.0842 - accuracy: 0.9890 - 71ms/epoch - 71ms/step\n",
            "1/1 - 0s - loss: 0.0520 - accuracy: 0.9919 - 95ms/epoch - 95ms/step\n",
            "1/1 - 0s - loss: 0.0545 - accuracy: 0.9931 - 72ms/epoch - 72ms/step\n",
            "1/1 - 0s - loss: 0.1070 - accuracy: 0.9867 - 74ms/epoch - 74ms/step\n",
            "1/1 - 0s - loss: 0.1568 - accuracy: 0.9827 - 74ms/epoch - 74ms/step\n",
            "1/1 - 0s - loss: 0.1306 - accuracy: 0.9850 - 76ms/epoch - 76ms/step\n",
            "1/1 - 0s - loss: 0.1576 - accuracy: 0.9815 - 98ms/epoch - 98ms/step\n",
            "1/1 - 0s - loss: 0.1504 - accuracy: 0.9809 - 75ms/epoch - 75ms/step\n",
            "1/1 - 0s - loss: 0.0784 - accuracy: 0.9896 - 79ms/epoch - 79ms/step\n",
            "1/1 - 0s - loss: 0.0482 - accuracy: 0.9936 - 71ms/epoch - 71ms/step\n",
            "1/1 - 0s - loss: 0.0806 - accuracy: 0.9890 - 107ms/epoch - 107ms/step\n",
            "1/1 - 0s - loss: 0.1206 - accuracy: 0.9827 - 80ms/epoch - 80ms/step\n",
            "1/1 - 0s - loss: 0.0690 - accuracy: 0.9902 - 74ms/epoch - 74ms/step\n",
            "1/1 - 0s - loss: 0.1074 - accuracy: 0.9890 - 73ms/epoch - 73ms/step\n",
            "1/1 - 0s - loss: 0.0842 - accuracy: 0.9873 - 103ms/epoch - 103ms/step\n",
            "1/1 - 0s - loss: 0.1821 - accuracy: 0.9786 - 71ms/epoch - 71ms/step\n",
            "1/1 - 0s - loss: 0.0426 - accuracy: 0.9942 - 77ms/epoch - 77ms/step\n",
            "1/1 - 0s - loss: 0.1710 - accuracy: 0.9798 - 76ms/epoch - 76ms/step\n",
            "1/1 - 0s - loss: 0.0799 - accuracy: 0.9896 - 80ms/epoch - 80ms/step\n",
            "1/1 - 0s - loss: 0.0941 - accuracy: 0.9884 - 88ms/epoch - 88ms/step\n",
            "1/1 - 0s - loss: 0.1109 - accuracy: 0.9896 - 79ms/epoch - 79ms/step\n",
            "1/1 - 0s - loss: 0.1403 - accuracy: 0.9821 - 79ms/epoch - 79ms/step\n",
            "1/1 - 0s - loss: 0.0431 - accuracy: 0.9931 - 87ms/epoch - 87ms/step\n",
            "1/1 - 0s - loss: 0.1333 - accuracy: 0.9844 - 80ms/epoch - 80ms/step\n",
            "1/1 - 0s - loss: 0.0993 - accuracy: 0.9861 - 75ms/epoch - 75ms/step\n",
            "1/1 - 0s - loss: 0.0798 - accuracy: 0.9902 - 79ms/epoch - 79ms/step\n",
            "1/1 - 0s - loss: 0.1137 - accuracy: 0.9850 - 81ms/epoch - 81ms/step\n",
            "1/1 - 0s - loss: 0.1425 - accuracy: 0.9827 - 76ms/epoch - 76ms/step\n",
            "1/1 - 0s - loss: 0.1246 - accuracy: 0.9873 - 75ms/epoch - 75ms/step\n",
            "1/1 - 0s - loss: 0.0986 - accuracy: 0.9902 - 91ms/epoch - 91ms/step\n",
            "1/1 - 0s - loss: 0.0601 - accuracy: 0.9908 - 77ms/epoch - 77ms/step\n",
            "1/1 - 0s - loss: 0.1344 - accuracy: 0.9850 - 97ms/epoch - 97ms/step\n",
            "1/1 - 0s - loss: 0.0950 - accuracy: 0.9890 - 76ms/epoch - 76ms/step\n",
            "1/1 - 0s - loss: 0.0871 - accuracy: 0.9884 - 81ms/epoch - 81ms/step\n",
            "1/1 - 0s - loss: 0.1196 - accuracy: 0.9890 - 84ms/epoch - 84ms/step\n",
            "1/1 - 0s - loss: 0.1000 - accuracy: 0.9884 - 139ms/epoch - 139ms/step\n",
            "1/1 - 0s - loss: 0.0827 - accuracy: 0.9884 - 79ms/epoch - 79ms/step\n",
            "1/1 - 0s - loss: 0.1417 - accuracy: 0.9844 - 76ms/epoch - 76ms/step\n",
            "1/1 - 0s - loss: 0.0558 - accuracy: 0.9913 - 76ms/epoch - 76ms/step\n",
            "1/1 - 0s - loss: 0.1319 - accuracy: 0.9821 - 99ms/epoch - 99ms/step\n",
            "1/1 - 0s - loss: 0.0809 - accuracy: 0.9896 - 76ms/epoch - 76ms/step\n",
            "1/1 - 0s - loss: 0.1514 - accuracy: 0.9850 - 86ms/epoch - 86ms/step\n",
            "1/1 - 0s - loss: 0.1277 - accuracy: 0.9861 - 72ms/epoch - 72ms/step\n",
            "1/1 - 0s - loss: 0.1337 - accuracy: 0.9832 - 70ms/epoch - 70ms/step\n",
            "1/1 - 0s - loss: 0.0347 - accuracy: 0.9925 - 98ms/epoch - 98ms/step\n",
            "1/1 - 0s - loss: 0.1494 - accuracy: 0.9809 - 76ms/epoch - 76ms/step\n",
            "1/1 - 0s - loss: 0.1189 - accuracy: 0.9861 - 72ms/epoch - 72ms/step\n",
            "1/1 - 0s - loss: 0.1643 - accuracy: 0.9798 - 74ms/epoch - 74ms/step\n",
            "1/1 - 0s - loss: 0.0863 - accuracy: 0.9896 - 151ms/epoch - 151ms/step\n",
            "1/1 - 0s - loss: 0.0777 - accuracy: 0.9925 - 79ms/epoch - 79ms/step\n",
            "1/1 - 0s - loss: 0.0908 - accuracy: 0.9879 - 74ms/epoch - 74ms/step\n",
            "1/1 - 0s - loss: 0.0725 - accuracy: 0.9867 - 71ms/epoch - 71ms/step\n",
            "1/1 - 0s - loss: 0.1063 - accuracy: 0.9861 - 75ms/epoch - 75ms/step\n",
            "1/1 - 0s - loss: 0.0737 - accuracy: 0.9902 - 75ms/epoch - 75ms/step\n",
            "1/1 - 0s - loss: 0.1688 - accuracy: 0.9821 - 72ms/epoch - 72ms/step\n",
            "1/1 - 0s - loss: 0.1603 - accuracy: 0.9809 - 75ms/epoch - 75ms/step\n",
            "1/1 - 0s - loss: 0.1322 - accuracy: 0.9827 - 77ms/epoch - 77ms/step\n",
            "1/1 - 0s - loss: 0.0665 - accuracy: 0.9879 - 96ms/epoch - 96ms/step\n",
            "1/1 - 0s - loss: 0.0874 - accuracy: 0.9902 - 71ms/epoch - 71ms/step\n",
            "1/1 - 0s - loss: 0.0356 - accuracy: 0.9942 - 73ms/epoch - 73ms/step\n",
            "1/1 - 0s - loss: 0.1225 - accuracy: 0.9861 - 72ms/epoch - 72ms/step\n",
            "1/1 - 0s - loss: 0.0969 - accuracy: 0.9890 - 114ms/epoch - 114ms/step\n",
            "1/1 - 0s - loss: 0.0430 - accuracy: 0.9931 - 70ms/epoch - 70ms/step\n",
            "1/1 - 0s - loss: 0.0810 - accuracy: 0.9896 - 76ms/epoch - 76ms/step\n",
            "1/1 - 0s - loss: 0.1172 - accuracy: 0.9879 - 72ms/epoch - 72ms/step\n",
            "1/1 - 0s - loss: 0.0414 - accuracy: 0.9925 - 75ms/epoch - 75ms/step\n",
            "1/1 - 0s - loss: 0.1437 - accuracy: 0.9844 - 111ms/epoch - 111ms/step\n",
            "1/1 - 0s - loss: 0.1039 - accuracy: 0.9850 - 67ms/epoch - 67ms/step\n",
            "1/1 - 0s - loss: 0.1464 - accuracy: 0.9838 - 78ms/epoch - 78ms/step\n",
            "1/1 - 0s - loss: 0.0653 - accuracy: 0.9925 - 77ms/epoch - 77ms/step\n",
            "1/1 - 0s - loss: 0.1126 - accuracy: 0.9884 - 130ms/epoch - 130ms/step\n",
            "1/1 - 0s - loss: 0.1177 - accuracy: 0.9873 - 80ms/epoch - 80ms/step\n",
            "1/1 - 0s - loss: 0.0967 - accuracy: 0.9884 - 66ms/epoch - 66ms/step\n",
            "1/1 - 0s - loss: 0.1815 - accuracy: 0.9815 - 79ms/epoch - 79ms/step\n",
            "1/1 - 0s - loss: 0.1136 - accuracy: 0.9867 - 70ms/epoch - 70ms/step\n",
            "1/1 - 0s - loss: 0.0892 - accuracy: 0.9884 - 76ms/epoch - 76ms/step\n",
            "Test Loss: 0.0966\n",
            "Test Accuracy: 0.9881\n"
          ]
        }
      ],
      "source": [
        "# Using a similar loop on the test set\n",
        "test_loss, test_accuracy = 0.0, 0.0\n",
        "num_of_test_samples = len(X_test)\n",
        "test_batch_size = 10\n",
        "\n",
        "for i in range(0, num_of_test_samples, test_batch_size):\n",
        "    start_idx = i\n",
        "    end_idx = min(i + test_batch_size, num_of_test_samples)\n",
        "\n",
        "    X_test_batch = X_test[start_idx:end_idx]\n",
        "    y_test_batch = y_test[start_idx:end_idx]\n",
        "\n",
        "    y_test_one_hot_batch = to_categorical(y_test_batch, num_classes=num_of_labels)\n",
        "\n",
        "    # Evaluating the model on the batch\n",
        "    batch_loss, batch_accuracy = my_model.evaluate(X_test_batch, y_test_one_hot_batch, verbose=2)\n",
        "\n",
        "    # Multiplying the the results of model.evaluate by the number of sequences in each batch\n",
        "    test_loss += batch_loss * (end_idx - start_idx)\n",
        "    test_accuracy += batch_accuracy * (end_idx - start_idx)\n",
        "\n",
        "# Averaging the metrics\n",
        "test_loss /= num_of_test_samples\n",
        "test_accuracy /= num_of_test_samples\n",
        "\n",
        "print(f\"Test Loss: {test_loss:.4f}\")\n",
        "print(f\"Test Accuracy: {test_accuracy:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5hCpR2Z0mY5J",
        "outputId": "f37573a3-8d73-45d1-9bd6-676dd1b503ff"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting arabic-reshaper\n",
            "  Downloading arabic_reshaper-3.0.0-py3-none-any.whl (20 kB)\n",
            "Installing collected packages: arabic-reshaper\n",
            "Successfully installed arabic-reshaper-3.0.0\n",
            "Collecting python-bidi\n",
            "  Downloading python_bidi-0.4.2-py2.py3-none-any.whl (30 kB)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from python-bidi) (1.16.0)\n",
            "Installing collected packages: python-bidi\n",
            "Successfully installed python-bidi-0.4.2\n"
          ]
        }
      ],
      "source": [
        "!pip install arabic-reshaper\n",
        "!pip install python-bidi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PmBkk2B0lfJG",
        "outputId": "46e60a3a-cea9-473e-a1c9-6924b9875257"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input Sequence: ﻣﻦ ﺍﻟﻤﻮﻗﻒ ﻫﺬﺍ ﻳﻘﻒ ﺇﺑﻠﻴﺲ ﷲ ﺟﻌﻞ ﻟﻘﺪ ﺃﺧﺮﻯ\n",
            "Predicted Unicode: ['1615 1618 1614', '1614 1614', '1614 1614', '1617 1614', '1616 1618 1616', '1614 1616', '1614 1614', '1618 1614 1618 1616', '1616']\n",
            "Predicted Arabic: ['ُ ْ َ', 'َ َ', 'َ َ', 'ّ َ', 'ِ ْ ِ', 'َ ِ', 'َ َ', 'ْ َ ْ ِ', 'ِ']\n",
            "\n",
            "Input Sequence: ﻋﻠﻴﻢ ﷲ ﺇﻥ ﺃﺗﻘﺎﻛﻢ ﷲ ﻋﻨﺪ ﺃﻛﺮﻣﻜﻢ ﺇﻥ ﻟﺘﻌﺎﺭﻓﻮﺍ ﻭﻗﺒﺎﺋﻞ ﺷﻌﻮﺑﺎ ﻭﺟﻌﻠﻨﺎﻛﻢ ﻭﺃﻧﺜﻰ ﺫﻛﺮ ﻣﻦ ﺧﻠﻘﻨﺎﻛﻢ ﺇﻧﺎ ﺍﻟﻨﺎﺱ ﺃﻳﻬﺎ ﻳﺎ ﺑﺎﻟﺘﻘﻮﻯ ﺍﻟﻘﺮﺁﻥ ﺣﺪﺩﻩ ﻣﺎ ﻭﻫﻮ ﻏﻴﺮﻩ\n",
            "Predicted Unicode: ['1614 1618 1616', '1614 1615', '1614', '1614 1617 1614 1614', '1618 1615 1618', '1616 1617 1614 1618 1614', '1614', '1614 1617 1615 1614', '1617 1614', '1616 1617 1614', '1614 1614 1618 1614 1615', '1616', '1614 1614', '1614 1615 1618 1614', '1614 1614 1614 1618 1614 1615', '1615 1615 1611', '1614 1614 1614 1616', '1616 1614 1614 1614 1615', '1616', '1614 1618 1614 1614 1615', '1616 1618', '1617 1614', '1614 1618 1614 1615', '1616', '1617 1614', '1614 1616']\n",
            "Predicted Arabic: ['َ ْ ِ', 'َ ُ', 'َ', 'َ ّ َ َ', 'ْ ُ ْ', 'ِ ّ َ ْ َ', 'َ', 'َ ّ ُ َ', 'ّ َ', 'ِ ّ َ', 'َ َ ْ َ ُ', 'ِ', 'َ َ', 'َ ُ ْ َ', 'َ َ َ ْ َ ُ', 'ُ ُ ً', 'َ َ َ ِ', 'ِ َ َ َ ُ', 'ِ', 'َ ْ َ َ ُ', 'ِ ْ', 'ّ َ', 'َ ْ َ ُ', 'ِ', 'ّ َ', 'َ ِ']\n",
            "\n",
            "Input Sequence: ﺍﻷﻃﻠﺴﻲ ﺍﻟﻤﺤﻴﻄﻴﻦ ﺑﻴﻦ ﺍﻷﺳﻄﻮﺭﻱ ﺍﻟﻤﻤﺮ ﺍﻛﺘﺸﺎﻑ ﺇﻟﻰ ﻓﺮﺩﺍ ﻣﻦ ﺍﻟﻤﻜﻮﻥ ﻭﻃﺎﻗﻤﻪ ﻫﻮ ﻳﺴﻌﻰ ﻓﺮﺍﻧﻜﻠﻴﻦ ﺟﻮﻥ ﺍﻟﺴﻴﺮ ﻭﻛﺎﻥ\n",
            "Predicted Unicode: ['1614 1614', '1617 1616', '1615', '0', '1614 1618 1614', '1615', '1614 1614 1614 1615', '1618 1615 1614 1617 1614', '1616', '1614 1618 1611', '1614', '1618 1616 1614', '1618 1614 1614', '1618 1615 1618 1615 1616', '1614 1618', '1618 1615 1616 1614 1618', '1618 1614 1618 1614 1616']\n",
            "Predicted Arabic: ['َ َ', 'ّ ِ', 'ُ', '\\x00', 'َ ْ َ', 'ُ', 'َ َ َ ُ', 'ْ ُ َ ّ َ', 'ِ', 'َ ْ ً', 'َ', 'ْ ِ َ', 'ْ َ َ', 'ْ ُ ْ ُ ِ', 'َ ْ', 'ْ ُ ِ َ ْ', 'ْ َ ْ َ ِ']\n",
            "\n",
            "Input Sequence: ﻳﻨﺘﻬﻲ ﻭﺃﻳﻦ\n",
            "Predicted Unicode: ['1614 1614 1614', '1614 1618 1614 1616']\n",
            "Predicted Arabic: ['َ َ َ', 'َ ْ َ ِ']\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#from arabic_reshaper import ArabicReshaper\n",
        "from bidi.algorithm import get_display\n",
        "#configuration = {'delete_harakat': False}\n",
        "#reshaper = ArabicReshaper(configuration=configuration)\n",
        "\n",
        "def decode_predictions(model, X_test, input_tokenizer, target_tokenizer, start_index, end_index):\n",
        "\n",
        "  def convert_to_arabic(tokens):\n",
        "      arabic_list = []\n",
        "      for token in tokens:\n",
        "          arabic_chars = [chr(int(codepoint)) for codepoint in token.split()]\n",
        "          arabic_list.append(' '.join(arabic_chars))\n",
        "      return arabic_list\n",
        "\n",
        "  for i in range(start_index - 1, end_index):  # -1 because the index starts at 0\n",
        "\n",
        "      # Not sure if I should use model.predict or model.predict_on_batch, results are similar\n",
        "      #prediction = model.predict(X_test[i:i + 1])[0]\n",
        "      prediction = model.predict_on_batch(X_test[i:i + 1])[0]\n",
        "\n",
        "      # Removing the padding zeros from the test sample and the prediction because they're annoying\n",
        "      input_sequence = X_test[i]\n",
        "      input_sequence = input_sequence[input_sequence != 0]\n",
        "      prediction = prediction[:len(input_sequence)]\n",
        "\n",
        "      # Getting the tokens corresponding to the integers in the sample vector\n",
        "      input_tokens = [input_tokenizer.index_word[idx] for idx in input_sequence]\n",
        "      target_tokens = [target_tokenizer.index_word[np.argmax(token)] for token in prediction]\n",
        "      target_arabic = convert_to_arabic(target_tokens)\n",
        "\n",
        "      # printing the input_sequence initially gave me left to right words\n",
        "      L2R_input_sequence = ' '.join(input_tokens)\n",
        "      # fixing this using Bidi:\n",
        "      input_sequence = get_display(L2R_input_sequence)\n",
        "\n",
        "\n",
        "      print(f\"Input Sequence: {input_sequence}\")\n",
        "      print (f\"Predicted Unicode: {target_tokens}\")\n",
        "      print( f\"Predicted Arabic: {target_arabic}\")\n",
        "      print()\n",
        "\n",
        "decode_predictions(my_model, X_test, bare_tokenizer, harakat_tokenizer, start_index=1333, end_index=1336)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
